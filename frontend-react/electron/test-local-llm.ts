/**
 * æœ¬åœ° LLM æœåŠ¡ç‹¬ç«‹æµ‹è¯•è„šæœ¬
 * ä½¿ç”¨ node-llama-cpp éªŒè¯ Qwen3-0.6B æ¨¡å‹æ¨ç†
 * 
 * è¿è¡Œ: npx tsx electron/test-local-llm.ts
 */

import path from 'path';
import { fileURLToPath } from 'url';

const __dirname = path.dirname(fileURLToPath(import.meta.url));

async function main() {
    console.log('ğŸš€ Loading node-llama-cpp...');

    const { getLlama, LlamaChatSession } = await import('node-llama-cpp');

    // è·å– Llama å®ä¾‹
    console.log('ğŸ”§ Initializing Llama (auto-detecting best backend)...');
    const llama = await getLlama();

    // è¾“å‡ºåç«¯ä¿¡æ¯
    console.log(`âœ… Backend: ${llama.gpu || 'CPU'}`);

    // æ¨¡å‹è·¯å¾„
    const modelPath = path.join(__dirname, '../models/qwen3-0.6b-q4_k_m.gguf');
    console.log(`ğŸ“¦ Loading model: ${modelPath}`);

    const startLoad = Date.now();
    const model = await llama.loadModel({ modelPath });
    console.log(`âœ… Model loaded in ${Date.now() - startLoad}ms`);

    // åˆ›å»ºä¸Šä¸‹æ–‡
    console.log('ğŸ”§ Creating context (4096 tokens)...');
    const context = await model.createContext({ contextSize: 4096 });

    // åˆ›å»ºèŠå¤©ä¼šè¯
    const session = new LlamaChatSession({
        contextSequence: context.getSequence(),
    });

    // æµ‹è¯•æ¨ç†
    console.log('\nğŸ’¬ Testing inference...\n');
    console.log('User: ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿè¯·ç®€çŸ­å›ç­”ã€‚\n');

    const startInfer = Date.now();
    let tokens = 0;

    console.log('Assistant: ');
    const response = await session.prompt('ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿè¯·ç®€çŸ­å›ç­”ã€‚', {
        onTextChunk: (chunk) => {
            process.stdout.write(chunk);
            tokens++;
        },
        maxTokens: 200,
    });

    const latency = Date.now() - startInfer;
    console.log(`\n\nğŸ“Š Stats: ${tokens} tokens in ${latency}ms (${(tokens / latency * 1000).toFixed(1)} tokens/s)`);

    // æ¸…ç†
    await context.dispose();
    await model.dispose();

    console.log('\nâœ… Test completed successfully!');
}

main().catch(console.error);
